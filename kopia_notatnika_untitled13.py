# -*- coding: utf-8 -*-
"""Kopia notatnika Untitled13.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1sLawjL5s9e5X0pWZ4_sLqnRN1OJAmqs-
"""

!pip install librosa pyAudioAnalysis SpeechRecognition openai-whisper transformers
!pip install opencv-python mediapipe deepface
!pip install spacy textstat langdetect
!pip install moviepy
!pip install dlib
!pip install noisereduce
!pip install python-Levenshtein
!apt install tesseract-ocr
!pip install pytesseract
!pip install vaderSentiment
!pip install eyed3

!pip install pydub
!python -m spacy download pl_core_news_sm

import librosa
import speech_recognition as sr
from pyAudioAnalysis import audioBasicIO
from pyAudioAnalysis import ShortTermFeatures
import whisper
from transformers import pipeline
import cv2
import mediapipe as mp
from deepface import DeepFace
import spacy
import textstat
from langdetect import detect
import moviepy.editor as mp_editor
import dlib
from moviepy.editor import VideoFileClip

import librosa
import speech_recognition as sr
from moviepy.editor import VideoFileClip
import cv2
from deepface import DeepFace
import textstat
from langdetect import detect
import numpy as np
import re
from textblob import TextBlob
import soundfile as sf
import os
from concurrent.futures import ThreadPoolExecutor
def extract_audio_from_video(video_path, audio_path):
    """Ekstrahuje audio z pliku wideo."""
    clip = VideoFileClip(video_path)
    clip.audio.write_audiofile(audio_path)

def transcribe_audio(audio_path):
    """Transkrybuje audio do tekstu."""
    recognizer = sr.Recognizer()
    with sr.AudioFile(audio_path) as source:
        audio_data = recognizer.record(source)
        try:
            return recognizer.recognize_google(audio_data, language='pl-PL')
        except sr.UnknownValueError:
            return "Nie udało się rozpoznać mowy"
        except sr.RequestError:
            return "Błąd żądania"

def analyze_emotions(video_path):
    """Analizuje emocje z klatek wideo."""
    cap = cv2.VideoCapture(video_path)
    emotions_list = []

    def process_frame(frame):
        try:
            result = DeepFace.analyze(frame, actions=['emotion'], enforce_detection=False)
            if result:
                return result[0]['dominant_emotion']
        except Exception as e:
            print("Błąd analizy emocji:", e)
        return None

    frames = []
    while cap.isOpened():
        ret, frame = cap.read()
        if not ret:
            break
        frames.append(frame)

    cap.release()

    # Użyj ThreadPoolExecutor do równoległej analizy
    with ThreadPoolExecutor() as executor:
        emotions_list = list(executor.map(process_frame, frames))

    return [emotion for emotion in emotions_list if emotion is not None]

def analyze_text(text, audio_path):
    """Analizuje tekst pod kątem zrozumiałości i błędów."""
    readability_score = textstat.flesch_reading_ease(text)
    gunning_fog_index = calculate_gunning_fog(text)
    language = detect(text)

    y, sr = librosa.load(audio_path, sr=None)
    duration = librosa.get_duration(y=y, sr=sr)

    errors = {
        "przerywniki": len(re.findall(r'\b(um|uh|no|like|tak|więc|wiesz|ej)\b', text)),
        "tempo": len(text.split()) / (duration / 60),
        "powtórzenia": sum(text.count(word) > 1 for word in set(text.split())),
        "liczby": len(re.findall(r'\d+', text)),
        "długie_słowa": sum(len(word) > 15 for word in text.split()),
        "trudne_słowa": len(re.findall(r'\b\w{12,}\b', text)),
        "obcy_język": language != 'pl',
        "za_długa_pauza": len(re.findall(r'\s{2,}', text)),
        "zmiana_tematu": len(re.findall(r'\b(teraz|przechodząc|wracając|w międzyczasie|na inny temat)\b', text)),
        "żargon": len(re.findall(r'\b(technologia|interfejs|algorytm|wymiana)\b', text)),
        "akcentowanie": len(re.findall(r'\b(nie|tak|to|właśnie|rzeczywiście)\b', text)),
        "trudne_zdania": len([sentence for sentence in re.split(r'[.!?]', text) if len(sentence.split()) > 20])
    }

    sentiment = TextBlob(text).sentiment

    return {
        'Wskaźnik czytelności': readability_score,
        'Indeks mglistości': gunning_fog_index,
        'Język': language,
        'Błędy': errors,
        'Sentyment': sentiment,
    }

def calculate_gunning_fog(text):
    """Oblicza współczynnik mglistości Gunninga."""
    sentences = len(re.split(r'[.!?]', text))
    words = len(text.split())
    difficult_words = len(re.findall(r'\b\w{3,}\b', text))

    if sentences == 0:
        return 0
    return 0.4 * ((words / sentences) + (difficult_words * 100 / words))

def analyze_volume(audio_path, threshold=0.1, lower_threshold=0.05):
    """Analizuje głośność audio."""
    y, sr = librosa.load(audio_path, sr=None)
    volume = np.mean(np.abs(y))

    if volume > threshold:
        return "Mówienie głośniej"
    elif volume < lower_threshold:
        return "Mówienie za cicho"
    return "Poziom głośności w normie"

def analyze_structure(text):
    """Ocena struktury wypowiedzi - wstęp, rozwinięcie, zakończenie."""
    sentences = re.split(r'[.!?]', text)
    if len(sentences) < 3:
        return "Brak pełnej struktury wypowiedzi."

    return {
        'Wstęp': sentences[0],
        'Rozwinięcie': ' '.join(sentences[1:-1]),
        'Zakończenie': sentences[-1]
    }

def detect_language(text):
    """Wykrywa użycie obcego języka."""
    language = detect(text)
    return language != 'pl'

def assess_target_audience(text):
    """Ocena grupy docelowej na podstawie tonu i używanych słów."""
    age_related_terms = re.findall(r'\b(temat|problem|wiedza|doświadczenie|dzieci|młodzież|dorośli|seniorzy)\b', text, re.IGNORECASE)
    if age_related_terms:
        return "Grupa docelowa zawiera odniesienia do: " + ', '.join(set(age_related_terms))
    return "Brak odniesień do grupy docelowej."

def analyze_transcription_differences(transcription, audio_path):
    """Analiza nieprawidłowości między wypowiedzią a transkrypcją."""
    transcription_words = transcription.split()

    y, sr = librosa.load(audio_path, sr=None)
    duration = librosa.get_duration(y=y, sr=sr)

    segments = int(np.ceil(duration / 5))
    audio_segments = [y[i * sr * 5 : (i + 1) * sr * 5] for i in range(segments)]

    recognized_words = []
    for i, segment in enumerate(audio_segments):
        temp_file = f'temp_segment_{i}.wav'
        sf.write(temp_file, segment, sr)  # Zapisuje fragment audio

        temp_recognized = transcribe_audio(temp_file)
        recognized_words.extend(temp_recognized.split())

        # Usuwanie pliku tymczasowego po użyciu
        os.remove(temp_file)

    recognized_set = set(recognized_words)
    transcription_set = set(transcription_words)

    missing_words = transcription_set - recognized_set
    extra_words = recognized_set - transcription_set

    analysis_result = {
        "Brakujące słowa": list(missing_words),
        "Dodatkowe słowa": list(extra_words),
        "Całkowita liczba rozpoznanych słów": len(recognized_words),
        "Całkowita liczba słów w transkrypcji": len(transcription_words),
        "Czas trwania audio (s)": duration
    }

    return analysis_result


video_path = "tu_zaciąg"
audio_path = "audio.wav"

extract_audio_from_video(video_path, audio_path)

text = transcribe_audio(audio_path)
print("Transkrypcja: ", text)

emotions_list = analyze_emotions(video_path)

summary = analyze_text(text, audio_path)

volume_analysis = analyze_volume(audio_path)

structure_analysis = analyze_structure(text)

target_audience_analysis = assess_target_audience(text)

transcription_analysis = analyze_transcription_differences(text, audio_path)

final_summary = {
    'Transkrypcja': text,
    'Emocje': emotions_list,
    **summary,
    'Analiza głośności': volume_analysis,
    'Analiza struktury': structure_analysis,
    'Analiza grupy docelowej': target_audience_analysis,
    'Analiza różnic w transkrypcji': transcription_analysis
}

print("\nPodsumowanie wyników:", final_summary)



video_path="tu_zaciąg"
clip=VideoFileClip(video_path)
audio_path = "audio.wav"
clip.audio.write_audiofile(audio_path)

y, sr = librosa.load(audio_path, sr=None)

recognizer = sr.Recognizer()
with sr.AudioFile(audio_path) as source:
    audio_data = recognizer.record(source)
    try:
        text = recognizer.recognize_google(audio_data, language='pl-PL')  # Użyj polskiego języka
    except sr.UnknownValueError:
        text = "Nie udało się rozpoznać mowy"
    except sr.RequestError:
        text = "Błąd żądania"

cap = cv2.VideoCapture(video_path)
while cap.isOpened():
    ret, frame = cap.read()
    if not ret:
        break



try:
        result = DeepFace.analyze(frame, actions=['emotion'], enforce_detection=False)
        emotions = result[0]['dominant_emotion']
        print("Emocje w klatce:", emotions)
    except Exception as e:
        print("Błąd analizy emocji:", e)


    cv2.imshow('Frame', frame)
    if cv2.waitKey(1) & 0xFF == ord('q'):
        break

cap.release()
cv2.destroyAllWindows()

language = detect(text)
print("Wykryty język:", language)

summary = {
    'Transkrypcja': text,
    'Emocje': emotions,
    'Wskaźnik czytelności': readability_score,
    'Język': language
}

print("\nPodsumowanie wyników:", summary)

tokenizer = Wav2Vec2Tokenizer.from_pretrained("allegro/herbert-base-cased")
model = Wav2Vec2ForCTC.from_pretrained("allegro/herbert-base-cased")

!pip install pytube

from pytube import YouTube

yt = YouTube("https://www.youtube.com/watch?v=rwD7m7nP64Y&list=RDrwD7m7nP64Y&start_radio=1")
audio_stream = yt.streams.filter(only_audio=True).first()
audio_stream.download(filename="audio.wav")

audio_input, _ = sf.read("https://www.youtube.com/watch?v=rwD7m7nP64Y&list=RDrwD7m7nP64Y&start_radio=1")

print(audio_input)

emotion=DeepFace.analyze(audio_input, actions=['emotion'])
foog= textstat_gunning_fog(audio_input)

y, _ = librosa.load('audio_file.wav')
tempo, _= librosa.beat.beat_track(y=y, sr=sr)
spectral= librosa.feature.spectral_centroid(y=y, sr=sr)

lp = spacy.load('en_core_web_sm')

doc = nlp(audio_input)
tokens = [token.text for token in doc]

tokens = word_tokenize(tekst)
zdania = sent_tokenize(tekst)
stop_words = set(stopwords.words('english'))



znaki_interpunkcyjne = ""
znaki_interpunkcyjne = znaki_interpunkcyjne + '\\n'

czestotliwosc_slow = {}
for slowo in doc:
  if slowo.text.lower() not in stop_words:
    if slowo.text.lower() not in znaki_interpunkcyjne:
      if slowo.text not in czestotliwosc_slow.keys():
        czestotliwosc_slow[slowo.text] = 1
      else:
        czestotliwosc_slow[slowo.text] += 1



model= models.Sequential()
model.add